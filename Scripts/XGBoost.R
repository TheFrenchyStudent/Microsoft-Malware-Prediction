if (!require("readr")) install.packages("readr"); library("readr")
if (!require("pROC")) install.packages("pROC"); library("pROC")
if (!require("dplyr")) install.packages("dplyr"); library("dplyr")
if (!require("xgboost")) install.packages("xgboost"); library("xgboost")
if (!require("caret")) install.packages("caret"); library("caret")
if (!require("MLmetrics")) install.packages("MLmetrics"); library("MLmetrics")
if (!require("Ckmeans.1d.dp")) install.packages("Ckmeans.1d.dp"); library("Ckmeans.1d.dp")



data <- readRDS("basetable_new_final.rds")

tokeep <- c("woe_AVProductStatesIdentifier","woe_AvSigVersion","woe_Census_SystemVolumeTotalCapacity","woe_SmartScreen",
            "woe_Census_InternalPrimaryDiagonalDisplaySizeInInches","woe_CountryIdentifier","woe_AVProductsInstalled",
            "Census_IsVirtualDevice","woe_OsBuildLab","woe_OsPlatformSubRelease",
            "Census_HasOpticalDiskDrive","woe_Census_OSBuildRevision","OsSuite",
            "woe_AppVersion","woe_Census_MDC2FormFactor","Census_IsSecureBootEnabled",
            "woe_LocaleEnglishNameIdentifier","woe_Census_OSWUAutoUpdateOptionsName","woe_Census_ActivationChannel",
            "AVProductsEnabled","woe_Census_ChassisTypeName","woe_Census_OSEdition",
            "woe_Census_FirmwareVersionIdentifier","woe_Census_FirmwareManufacturerIdentifier","RtpStateBitfield",
            "SMode","woe_Census_OSInstallTypeName" ,"woe_Census_FlightRing",
            "woe_Census_ProcessorCoreCount","Census_ProcessorClass","woe_Platform",
            "woe_Census_PrimaryDiskTotalCapacity","woe_Census_TotalPhysicalRAM","woe_Census_OSArchitecture",
            "woe_Census_OSBuildNumber", "HasDetections")


# Subset with most relevant features
data <- data[tokeep]
data$HasDetections <- as.factor(data$HasDetections)

# Smaller subset for grid-search
size_subset <- 1*nrow(data)
subset_i <- sample(1:nrow(data), size_subset)
subset <- data[subset_i,]

# Train-test split
train_i <- sample(1:nrow(subset), .8*nrow(subset))
train <- subset[train_i,]
test <- subset[-train_i,]


# Transform dataframes into a numeric matrix (format required by xgb)
X_train <- data.matrix(train[,names(train) != "HasDetections"])
y_train <- as.numeric(train$HasDetections) - 1

X_test <- data.matrix(test[,names(test) != "HasDetections"])
y_test <- test$HasDetections


### Grid-search
results <- matrix(0, ncol=7, nrow=54)
results <- data.frame(results)
colnames(results) <- c("nrounds", "eta", "gamma", "sample_proportion", "auc", "accuracy","training_time")

# Parameter grid
param_nrounds <- c(50, 100, 200)
param_eta <- c(0.1, 0.3, 0.5)
param_gamma <- c(0, 1, 10)
param_samples <- c(0.5, 1)

i <- 1
for (n in param_nrounds){
  for (e in param_eta){
    for (g in param_gamma){
      for (s in param_samples){
        
        start_time <- Sys.time()
        
        # Modeling
        xgb <- xgboost(data = X_train, 
                       label = y_train, 
                       eta = e,
                       gamma = g,
                       nround=n, 
                       subsample = s,
                       colsample_bytree = s,
                       objective = "binary:logistic"
        )
        
        # Time spent fitting the model
        end_time <- Sys.time()
        training_time <- round(end_time - start_time, digits=2)
        
        # Evaluation
        proba_preds <- predict(xgb, X_test)
        preds <- as.numeric(proba_preds > 0.5)
        auc_score <- auc(y_test, preds)
        accu_score <- Accuracy(y_test, preds)
        
        # nrounds
        results[i, 1] <- n
        # eta (learning rate)
        results[i, 2] <- e
        # gamma
        results[i, 3] <- g
        # sample proportion
        results[i, 4] <- s
        # auc
        results[i, 5] <- auc_score
        # accuracy
        results[i, 6] <- accu_score
        # Training time
        results[i, 7] <- training_time
        
        
        print(paste0("Finished model ", i, "/54, nrounds=", n, ", training time=", training_time))
        i = i+1
      }
    }
  }
}

# Grid-search results
sorted_results <- arrange(results, by=desc(auc))
write.csv(sorted_results, "results_grid_search_xgb.csv")



### Cross-validation for the best set of parameters
sorted_results[1:3,]

n <- sorted_results[1,1]
e <- sorted_results[1,2]
g <- sorted_results[1,3]
s <- sorted_results[1,4]
  
# param_set <- list(eta = 0.1, gamma = 0, subsample = 0.5, colsample_bytree = 0.5, nround=200)

folds <- createFolds(subset$HasDetections, k = 10, list = TRUE, returnTrain = FALSE)

i = 1
auc_cv <- c()
accu_cv <- c()
for (fold in folds){
  train <- subset[-fold,]
  test <- subset[fold,]
  X_train <- data.matrix(train[,names(train) != "HasDetections"])
  y_train <- as.numeric(train$HasDetections) - 1
  X_test <- data.matrix(test[,names(test) != "HasDetections"])
  y_test <- test$HasDetections
  
  # Modeling
  xgb <- xgboost(data = X_train, 
                 label = y_train, 
                 eta = e,
                 gamma = g,
                 nround = n, 
                 subsample = s,
                 colsample_bytree = s,
                 objective = "binary:logistic"
  )

  # Evaluation
  proba_preds <- predict(xgb, X_test)
  preds <- as.numeric(proba_preds > 0.5)
  auc_score <- auc(y_test, preds)
  accu_score <- Accuracy(test$HasDetections, preds)
  auc_cv <- c(auc_cv, auc_score)
  accu_cv <- c(accu_cv, accu_score)
  
  print(paste0("CV ",i, "/", length(folds)))
  
  i = i+1
} 

print(paste0("XGBoost with nrounds=",n," ,gamma=",g," ,learning rate=",e, " ,sample proportion=",s))
print(paste0("CV AUC score:", mean(auc_cv)))
print(paste0("CV Accuracy score:", mean(accu_cv)))


# Variable importance

xgb <- xgboost(data = X_train, 
               label = y_train, 
               eta = 0.3,
               gamma = 10,
               nround = 200, 
               subsample = 0.5,
               colsample_bytree = 0.5,
               objective = "binary:logistic"
)

imp <- xgb.importance(model=xgb)

xgb.ggplot.importance(imp,top_n=10)
